{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import ml_project as ml\n",
    "import time\n",
    "from sklearn.decomposition import PCA\n",
    "csv_data = pd.read_csv(\"adult.data.txt\", skipinitialspace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This dataset is used to predict whether a person's annual income is >50K or <=50K\n",
    "\n",
    "These are the fetures we'll use:\n",
    "\n",
    "age: continuous.\n",
    "\n",
    "workclass: Private, Self-emp-not-inc, Self-emp-inc, Federal-gov, Local-gov, State-gov, Without-pay, Never-worked.\n",
    "\n",
    "fnlwgt: continuous.\n",
    "\n",
    "education: Bachelors, Some-college, 11th, HS-grad, Prof-school, Assoc-acdm, Assoc-voc, 9th, 7th-8th, 12th, Masters, 1st-4th, 10th, Doctorate, 5th-6th, Preschool.\n",
    "\n",
    "education-num: continuous.\n",
    "\n",
    "marital-status: Married-civ-spouse, Divorced, Never-married, Separated, Widowed, Married-spouse-absent, Married-AF-spouse.\n",
    "\n",
    "occupation: Tech-support, Craft-repair, Other-service, Sales, Exec-managerial, Prof-specialty, Handlers-cleaners, Machine-op-inspct, Adm-clerical, Farming-fishing, Transport-moving, Priv-house-serv, Protective-serv, Armed-Forces.\n",
    "\n",
    "relationship: Wife, Own-child, Husband, Not-in-family, Other-relative, Unmarried.\n",
    "\n",
    "race: White, Asian-Pac-Islander, Amer-Indian-Eskimo, Other, Black.\n",
    "\n",
    "sex: Female, Male.\n",
    "\n",
    "capital-gain: continuous.\n",
    "\n",
    "capital-loss: continuous.\n",
    "\n",
    "hours-per-week: continuous.\n",
    "\n",
    "native-country: United-States, Cambodia, England, Puerto-Rico, Canada, Germany, Outlying-US(Guam-USVI-etc), India, Japan, Greece, South, China, Cuba, Iran, Honduras, Philippines, Italy, Poland, Jamaica, Vietnam, Mexico, Portugal, Ireland, France, Dominican-Republic, Laos, Ecuador, Taiwan, Haiti, Columbia, Hungary, Guatemala, Nicaragua, Scotland, Thailand, Yugoslavia, El-Salvador, Trinadad&Tobago, Peru, Hong, Holand-Netherlands."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here's what the fist few samples look like\n",
    "csv_data.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to use this to do classification with POLK and deep nets we'll transform all of these features to be vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for simplicity, we'll toss out any sample that contains an unknown value represented as '?'\n",
    "drop_inds = []\n",
    "for i, sample in csv_data.iterrows():\n",
    "    for val in sample:\n",
    "        if val == '?':\n",
    "            drop_inds.append(i)\n",
    "csv_data = csv_data.drop(drop_inds)            \n",
    "\n",
    "# next, we need to transform nominal variables that are represented as strings into a numerical value\n",
    "# we'll use some dictionaries based on the descriptions\n",
    "\n",
    "# just drop the education column b/c it is equivalent to education_num\n",
    "csv_data = csv_data.drop(columns=['education'])\n",
    "\n",
    "replace_dict = {\n",
    "    'workclass':{\n",
    "        'Private':0, \n",
    "        'Self-emp-not-inc':1, \n",
    "        'Self-emp-inc':2, \n",
    "        'Federal-gov':3, \n",
    "        'Local-gov':4, \n",
    "        'State-gov':5, \n",
    "        'Without-pay':6, \n",
    "        'Never-worked':7\n",
    "    },\n",
    "    'marital-status':{\n",
    "        'Married-civ-spouse':0,\n",
    "        'Divorced':1,\n",
    "        'Never-married':2,\n",
    "        'Separated':3,\n",
    "        'Widowed':4,\n",
    "        'Married-spouse-absent':5, \n",
    "        'Married-AF-spouse':6\n",
    "    },\n",
    "    'occupation':{\n",
    "        'Tech-support':0,\n",
    "        'Craft-repair':1,\n",
    "        'Other-service':2,\n",
    "        'Sales':3,\n",
    "        'Exec-managerial':4,\n",
    "        'Prof-specialty':5,\n",
    "        'Handlers-cleaners':6,\n",
    "        'Machine-op-inspct':7,\n",
    "        'Adm-clerical':8,\n",
    "        'Farming-fishing':9,\n",
    "        'Transport-moving':10,\n",
    "        'Priv-house-serv':11,\n",
    "        'Protective-serv':12,\n",
    "        'Armed-Forces':13\n",
    "    },\n",
    "    'relationship':{\n",
    "        'Wife':0,\n",
    "        'Own-child':1,\n",
    "        'Husband':2,\n",
    "        'Not-in-family':3,\n",
    "        'Other-relative':4,\n",
    "        'Unmarried':5\n",
    "    },\n",
    "    'race':{\n",
    "        'White':0,\n",
    "        'Asian-Pac-Islander':1,\n",
    "        'Amer-Indian-Eskimo':2,\n",
    "        'Other':3,\n",
    "        'Black':4\n",
    "    },\n",
    "    'sex':{\n",
    "        'Female':0,\n",
    "        'Male':1\n",
    "    },\n",
    "    'native-country':{\n",
    "        'United-States':0,\n",
    "        'Cambodia':1,\n",
    "        'England':2,\n",
    "        'Puerto-Rico':3,\n",
    "        'Canada':4,\n",
    "        'Germany':5,\n",
    "        'Outlying-US(Guam-USVI-etc)':6,\n",
    "        'India':7,\n",
    "        'Japan':8,\n",
    "        'Greece':9,\n",
    "        'South':10,\n",
    "        'China':11,\n",
    "        'Cuba':12,\n",
    "        'Iran':13,\n",
    "        'Honduras':14,\n",
    "        'Philippines':15,\n",
    "        'Italy':16,\n",
    "        'Poland':17,\n",
    "        'Jamaica':18,\n",
    "        'Vietnam':19,\n",
    "        'Mexico':20,\n",
    "        'Portugal':21,\n",
    "        'Ireland':22,\n",
    "        'France':23,\n",
    "        'Dominican-Republic':24,\n",
    "        'Laos':25,\n",
    "        'Ecuador':26,\n",
    "        'Taiwan':27,\n",
    "        'Haiti':28,\n",
    "        'Columbia':29,\n",
    "        'Hungary':30,\n",
    "        'Guatemala':31,\n",
    "        'Nicaragua':32,\n",
    "        'Scotland':33,\n",
    "        'Thailand':34,\n",
    "        'Yugoslavia':35,\n",
    "        'El-Salvador':36,\n",
    "        'Trinadad&Tobago':37,\n",
    "        'Peru':38,\n",
    "        'Hong':39,\n",
    "        'Holand-Netherlands':40\n",
    "    },\n",
    "    'class':{\n",
    "        '<=50K':0,\n",
    "        '>50K':1\n",
    "    }\n",
    "}\n",
    "csv_data = csv_data.replace(replace_dict)\n",
    "labels = csv_data['class']\n",
    "data = csv_data.drop(columns=['class'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll normalize all the features by taking the column-wise mean and subtracting it from each value, then dividing by the standard deviation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.transform(lambda x: (x-x.mean())-x.std())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When breaking into training and test we need make some changes to account for the class imbalance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = len(labels)\n",
    "data = np.array(data)\n",
    "labels  = np.array(labels)\n",
    "# find the indices of each class\n",
    "zero_inds = np.where(labels==0)[0]\n",
    "one_inds = np.where(labels)[0]\n",
    "data_0, data_1 = data[zero_inds], data[one_inds]\n",
    "label_0, label_1 = labels[zero_inds], labels[one_inds]\n",
    "# we want an even split of each class. Since there are fewer one's we'll split those and randomly sample the same number of zeros\n",
    "N = len(one_inds)\n",
    "train_slice = int(.8 * N)\n",
    "test_slice = N - train_slice\n",
    "# preallocate\n",
    "train_data = np.zeros((2*train_slice, data.shape[1]))\n",
    "test_data = np.zeros((2*test_slice, data.shape[1]))\n",
    "# split up the ones\n",
    "train_data[:train_slice,:] = data_1[:train_slice,:]\n",
    "test_data[test_slice:,:] = data_1[train_slice:,:]\n",
    "# randomly choose as many zeros as we have ones\n",
    "chosen_inds = np.random.choice(zero_inds,N,replace=False)\n",
    "# split them between training and test\n",
    "train_data[train_slice:,:] = data[chosen_inds[:train_slice],:]\n",
    "test_data[test_slice:,:] = data[chosen_inds[train_slice:],:]\n",
    "# put the labels together\n",
    "train_labels = np.zeros(2*train_slice)\n",
    "train_labels[:train_slice] = np.ones(train_slice)\n",
    "test_labels = np.zeros(2*test_slice)\n",
    "test_labels[:test_slice] = np.ones(test_slice)\n",
    "# shuffle everything\n",
    "np.random.seed(101)\n",
    "np.random.shuffle(train_data)\n",
    "np.random.seed(101)\n",
    "np.random.shuffle(train_labels)\n",
    "np.random.seed(202)\n",
    "np.random.shuffle(test_data)\n",
    "np.random.seed(202)\n",
    "np.random.shuffle(test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('train_data',train_data)\n",
    "np.save('train_labels',train_labels)\n",
    "np.save('test_data',test_data)\n",
    "np.save('test_labels',test_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = np.load('train_data.npy')\n",
    "train_labels = np.load('train_labels.npy')\n",
    "test_data = np.load('test_data.npy')\n",
    "test_labels = np.load('test_labels.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Next, we'll define a function to train a neural net to classify this data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_mlp(step_size, num_layers, epochs, data):\n",
    "    train_data, train_labels, test_data, test_labels = data\n",
    "    print('*************************************\\ntraining mlp with {} layers and step_size={}...'.format(num_layers,step_size))\n",
    "    tf.reset_default_graph()\n",
    "    sess = tf.Session()\n",
    "\n",
    "    inputs = tf.placeholder(tf.float64, shape=(None,train_data.shape[1]),name='input_placeholder')\n",
    "    labels = tf.placeholder(dtype=tf.int32, shape=(None,))\n",
    "    one_hot_labels = tf.one_hot(labels,2)\n",
    "\n",
    "    h = inputs\n",
    "\n",
    "    for layer in range(num_layers):\n",
    "        h = tf.contrib.layers.fully_connected(inputs=h, num_outputs=12, weights_regularizer=tf.nn.l2_loss)\n",
    "    h = tf.contrib.layers.fully_connected(inputs=h, num_outputs=2, activation_fn=None)\n",
    "\n",
    "    regularizer_loss = tf.losses.get_regularization_loss()\n",
    "    alpha = 1e-6\n",
    "    loss = tf.reduce_mean(tf.losses.sigmoid_cross_entropy(multi_class_labels=one_hot_labels, logits=h)) + alpha * regularizer_loss\n",
    "\n",
    "#     optimizer = tf.train.GradientDescentOptimizer(learning_rate=.001)\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=step_size)\n",
    "    minimizer = optimizer.minimize(loss)\n",
    "\n",
    "    prediction = tf.argmax(h,1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(tf.equal(prediction, tf.cast(labels,tf.int64)),tf.float64))\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    print( \"Total number of variables used \", np.sum([v.get_shape().num_elements() for v in tf.trainable_variables()]) )\n",
    "    BS = 10\n",
    "    for epoch in range(epochs):\n",
    "        np.random.seed(epoch)\n",
    "        np.random.shuffle(train_data)\n",
    "        np.random.seed(epoch)\n",
    "        np.random.shuffle(train_labels)\n",
    "        train_losses = []\n",
    "        test_losses = []\n",
    "        train_errs = []\n",
    "        test_errs = []\n",
    "        step_times = []\n",
    "        for i in range(0, train_data.shape[0]-BS+1, BS):\n",
    "            batch_data, batch_labels = train_data[i:i+BS], train_labels[i:i+BS]\n",
    "            start = time.time()\n",
    "            _, _train_loss, _train_acc = sess.run([minimizer, loss, accuracy], feed_dict={inputs: batch_data, labels: batch_labels})\n",
    "            end = time.time()\n",
    "            # update training stats\n",
    "            train_losses.append(_train_loss)\n",
    "            train_errs.append(1-_train_acc)\n",
    "            step_times.append(end-start)\n",
    "            # update test stats\n",
    "            test_loss, test_acc, pred_vec = sess.run([loss, accuracy, prediction], feed_dict={inputs: test_data, labels: test_labels})\n",
    "            test_losses.append(test_loss)\n",
    "            test_errs.append(1-test_acc)\n",
    "    # print('Train loss: {:.3f}. Training Acc: {:.3f}. Test loss: {:.3f}. Test Acc: {:.3f}.'.format(np.mean(train_losses), np.mean(train_accs), test_loss, test_acc))\n",
    "    print('Test accuracy: {:.3f}.'.format(test_acc))\n",
    "    print('final confusion matrix:')\n",
    "    TP = np.sum(np.logical_and(pred_vec == 1, test_labels == 1))\n",
    "    FP = np.sum(np.logical_and(pred_vec == 1, test_labels == 0))\n",
    "    TN = np.sum(np.logical_and(pred_vec == 0, test_labels == 0))\n",
    "    FN = np.sum(np.logical_and(pred_vec == 0, test_labels == 1))\n",
    "    print('TP: {}. FP: {}.\\nTN: {}. FN: {}.'.format(TP,FP,TN,FN))\n",
    "    return train_losses, test_losses, train_errs, test_errs, step_times"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we want to try the same thing with POLK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_POLK(step_size, sigma, eps, data):\n",
    "    train_data, train_labels, test_data, test_labels = data\n",
    "    train_losses = []\n",
    "    test_losses = []\n",
    "    train_errors = []\n",
    "    test_errors = []\n",
    "    model_orders = []\n",
    "    step_times = []\n",
    "    BS = 10\n",
    "    kernel = ml.gaussian_kernel(sigma)\n",
    "    model = ml.sklr_model(kernel, 1e-9, eps)\n",
    "    sgd = ml.SGD(model)\n",
    "    print('*********************')\n",
    "    print('training POLK with step szie: {}. sigma: {}. error threshold: {}. '.format(step_size, sigma, eps))\n",
    "    for e in range(epochs):\n",
    "        print('epoch: ', e)\n",
    "        epoch_start = time.time()\n",
    "        seed = e\n",
    "        np.random.seed(seed)\n",
    "        np.random.shuffle(train_data)\n",
    "        np.random.seed(seed)\n",
    "        np.random.shuffle(train_labels)\n",
    "        for i in range(0, train_data.shape[0], BS):\n",
    "            start = time.time()\n",
    "            sgd.fit(step_size, train_data[i:i+BS], train_labels[i:i+BS])\n",
    "            end = time.time()\n",
    "            # add the time to compute sgd\n",
    "            step_times.append(end-start)\n",
    "            # calcualte training and test loss\n",
    "            train_losses.append(model.loss(train_data, train_labels))\n",
    "            test_losses.append(model.loss(test_data, test_labels))\n",
    "            # calculate training accuracy\n",
    "            predictions = model.predict(train_data) >= .5\n",
    "            train_labels.shape = predictions.shape\n",
    "            correct = (predictions == train_labels).sum()\n",
    "            train_errors.append(1 - (correct/(train_labels.shape[0])))\n",
    "            # calculate test accuracy\n",
    "            predictions = model.predict(test_data) >= .5\n",
    "            test_labels.shape = predictions.shape\n",
    "            correct = (predictions == test_labels).sum()\n",
    "            test_errors.append(1 - (correct/(test_labels.shape[0])))\n",
    "            # add the current model order\n",
    "            model_orders.append(model.dictionary().shape[0])\n",
    "        epoch_end = time.time()\n",
    "        print('time to run epoch: {} seconds'.format(epoch_end - epoch_start))\n",
    "        print('training loss: {}. test loss: {}'.format(train_losses[-1],test_losses[-1]))\n",
    "        print('model order: ',model.dictionary().shape[0])\n",
    "        print('test error: {}'.format(test_errors[-1]))\n",
    "    return train_losses, test_losses, train_errors, test_errors, step_times, model_orders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# both methods will train over a series of batch sizes\n",
    "step_sizes = [1,.5,.3,.1,.03,.01,.003,.001,.0003,.0001,.00003,.00001]\n",
    "epochs = 30\n",
    "data = (train_data, train_labels, test_data, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# will be indexed by (num_layers, step_size)\n",
    "mlp_stats = {}\n",
    "layers = range(2,6)\n",
    "for num_layers in layers:\n",
    "    for step_size in step_sizes:\n",
    "        result = train_mlp(step_size, num_layers, epochs, data)\n",
    "        train_losses, test_losses, train_errors, test_errors, step_times = result\n",
    "        stat = {}\n",
    "        stat['train_loss'] = train_losses\n",
    "        stat['test_loss'] = test_losses\n",
    "        stat['train_errors'] = train_errors\n",
    "        stat['test_errors'] = test_errors\n",
    "        stat['step_times'] = step_times\n",
    "        mlp_stats[num_layers,step_size] = stat\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# will be indexed by (epsilon-error, sigma, step_size)\n",
    "polk_stats = {}\n",
    "epsilons = [.1,.01,.001,.0009,.0008,.0007,.0006]\n",
    "sigmas = np.arange(.1,1.1,.1)\n",
    "for eps in epsilons:\n",
    "    for sigma in sigmas:\n",
    "        for step_size in step_sizes:\n",
    "            result = train_POLK(step_size, sigma, eps, data)\n",
    "            train_losses, test_losses, train_errors, test_errors, step_times, model_orders = result\n",
    "            stat = {}\n",
    "            stat['train_loss'] = train_losses\n",
    "            stat['test_loss'] = test_losses\n",
    "            stat['train_errors'] = train_errors\n",
    "            stat['test_errors'] = test_errors\n",
    "            stat['step_times'] = step_times\n",
    "            stat['model_order'] = model_orders\n",
    "            plok_stats[eps,sigma,step_size] = stat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
